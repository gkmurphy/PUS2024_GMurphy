{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv73mQWiBdtPNEBNYvqjdK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkmurphy/PUS2024_GMurphy/blob/ClassDemos/Transformers(Willow).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time Series Classifications with Transformers via KERAS\n",
        "##*The Great Wrong-doing by Willow*\n",
        "\n",
        "Transformers are a neural network using multi-layer attention layers. These layers are performing a series of computations (activation functions and weights). Botswani (2017) *Attention is All you Need* - attention mechanisms are essentially the backbone for chatgbt by finding relationships to words within a sentence. The attention mechanism find relationships between elements in the array by transforming the word to numbers that can then be associated.\n",
        "\n",
        "It's super great in natural language processing, but is so nifty and great that people want to apply it to all the things - including classification problems - which are not directly applicable to this type of methodology.\n",
        "\n",
        "Positional encoding is pretty sweet too and isn't used for categorical."
      ],
      "metadata": {
        "id": "3Xru-yjDXzYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJMsCvf2XfBX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
        "    y = data[:, 0]\n",
        "    x = data[:, 1:]\n",
        "    return x, y.astype(int)\n",
        "\n",
        "\n",
        "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\" #data is hosted here ; extracted from here\n",
        "\n",
        "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
        "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")"
      ],
      "metadata": {
        "id": "765hY2hQZu5M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape #This dataset is a set of engine recordings = 3601 samples\n",
        "np.unique(y_train) #engine is bueno or no bueno"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCP91ZFvZ1uc",
        "outputId": "3438128e-ea2b-4ab5-ae83-6d0a37ddf67b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[...] #ellipses means include all other dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkJ_VGTDacKE",
        "outputId": "ad0f7001-cea8-43b6-b7f1-d6a325c835cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.79717168, -0.66439208, -0.37301463, ..., -0.66439208,\n",
              "        -1.0737958 , -1.5643427 ],\n",
              "       [ 0.80485472,  0.63462859,  0.37347448, ..., -0.71488505,\n",
              "        -0.56044294, -0.31908642],\n",
              "       [ 0.7279851 ,  0.11128392, -0.49912439, ...,  0.39446303,\n",
              "         0.33940042,  0.25539062],\n",
              "       ...,\n",
              "       [-0.57005428, -0.33316523, -0.29351853, ..., -1.3937145 ,\n",
              "        -0.94273327, -0.27072168],\n",
              "       [ 2.0067321 ,  2.0791499 ,  2.0220362 , ..., -0.43214504,\n",
              "        -0.44123126, -0.28070891],\n",
              "       [-0.12524091, -0.32536268, -0.48823697, ...,  0.55576053,\n",
              "         0.57445102,  0.57311598]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train[:,:, None] #these do the same thing ; NONE just adds another array/dimensions\n",
        "x_test = x_test[..., None]\n",
        "\n",
        "x_train.shape, x_test.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO_FrZjBaK0E",
        "outputId": "37f36187-427f-417e-ccfb-93d421018051"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3601, 500, 1), (1320, 500, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = len(np.unique(y_train))\n",
        "n_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oaWXG5Va50D",
        "outputId": "3c442103-4639-4583-e50f-339da8e90bf2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomize a dataset\n",
        "idx = np.random.permutation(len(x_train))#length of x_train is unclear because it has multiple dimensions\n",
        "idx = np.random.permutation(x_train.shape[0])\n",
        "\n",
        "\n",
        "x_train = x_train[idx] #randomized index - data is still the same\n",
        "y_train = y_train[idx]\n",
        "\n",
        "y_train[y_train == -1] = 0 #changing this for convention so it's -1 and 0 so classification options are sequential ; keras likes that\n",
        "y_test[y_test == -1] = 0"
      ],
      "metadata": {
        "id": "v483ej9LbAxZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You would have to do this if you didn't install the nlp above\n",
        "def transformer_encoder(inputs):\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(inputs) #this must go first for reasons unknown\n",
        "  x = layers.MultiHeadAttention(key_dim=8, num_heads=8, dropout=0)(x, x) #it just takes two inputs.. weird ; #pass it the previous input as the input\n",
        "  res = x + inputs #add the inputs and outputs together to preserve the original data throughout the process\n",
        "\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  x = layers.Conv1D(filters=2000, kernel_size = 1, activation= \"relu\")(x)\n",
        "  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "\n",
        "  return res + x\n",
        "\n",
        "\n",
        "def build_model(input_shape, n_classes):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  x = inputs\n",
        "\n",
        "  for _ in range(2): #underscore is used as a dummy variable to tell you to loop thru it 2x\n",
        "    x = transformer_encoder(x) #the first time it happens - the input will be the output from the last time ; essentially runs this line twice\n",
        "\n",
        "  x = layers.GlobalMaxPooling1D(data_format = \"channels_first\")(x)\n",
        "  x = layers.Dense(78, activation=\"relu\")(x)\n",
        "  x = layers.Dense(50, activation = \"relu\")(x)\n",
        "\n",
        "  outputs = layers.Dense(n_classes, activation = \"softmax\")(x)\n",
        "\n",
        "  model = keras.Model(inputs, outputs) #add the inputs and outputs\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "p9p1uVvMbx7a"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "model = build_model(input_shape, n_classes)"
      ],
      "metadata": {
        "id": "l_wfWnHvgef_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "id": "oxGsa1F-h2xt",
        "outputId": "16721075-b705-4db0-aec5-25a0029cf8b0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_8     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m2\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_4    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │            \u001b[38;5;34m449\u001b[0m │ layer_normalization_8… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_8… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_9     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m2\u001b[0m │ multi_head_attention_… │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m2000\u001b[0m)      │          \u001b[38;5;34m4,000\u001b[0m │ layer_normalization_9… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ input_layer_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2,001\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_9 (\u001b[38;5;33mAdd\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
              "│                           │                        │                │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_10    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m2\u001b[0m │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │            \u001b[38;5;34m449\u001b[0m │ layer_normalization_1… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_11    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m2\u001b[0m │ multi_head_attention_… │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m2000\u001b[0m)      │          \u001b[38;5;34m4,000\u001b[0m │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_10 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2,001\u001b[0m │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_11 (\u001b[38;5;33mAdd\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],          │\n",
              "│                           │                        │                │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m78\u001b[0m)             │         \u001b[38;5;34m39,078\u001b[0m │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │          \u001b[38;5;34m3,950\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m102\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_8     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_4    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">449</span> │ layer_normalization_8… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_8… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_9     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ multi_head_attention_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> │ layer_normalization_9… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
              "│                           │                        │                │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_10    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ multi_head_attention_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">449</span> │ layer_normalization_1… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)      │                        │                │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ layer_normalization_11    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ multi_head_attention_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2000</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,000</span> │ layer_normalization_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_attention_… │\n",
              "│                           │                        │                │ add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,001</span> │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],          │\n",
              "│                           │                        │                │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_max_pooling1d_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">78</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">39,078</span> │ global_max_pooling1d_… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">3,950</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,038\u001b[0m (218.90 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,038</span> (218.90 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,038\u001b[0m (218.90 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,038</span> (218.90 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = \"sparse_categorical_crossentropy\", #sparse means it's one hot encoded\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics = [\"sparse_categorical_accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "9Y-vUhGchqAA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = [\n",
        "    keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True) #stop running after 10 epochs if it's not good then turn back to the best weights at that point\n",
        "]"
      ],
      "metadata": {
        "id": "6zgBgY5uh_qI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_split = 0.2, #add validation to set aside some of the variation to evaluate it on data it's never seen before\n",
        "    epochs = 100,\n",
        "    batch_size = 32, #batch is how many examples you're giving the model to run at once ; smaller helps prevent overfitting (only seeing/learning a small part of the variation at once - higher variability)\n",
        "                     #learning not to be too specific (overfitting) but still not too low so it converges faster\n",
        "    verbose = 2  #how much info the model should print out while it's running ; smaller progress bars are better if you want to save it into a file\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyRSh2x1iOEz",
        "outputId": "4f4ac58f-752a-41f2-d1fb-e0aecc03ba10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "90/90 - 509s - 6s/step - loss: 0.6099 - sparse_categorical_accuracy: 0.6660 - val_loss: 0.6343 - val_sparse_categorical_accuracy: 0.6449\n",
            "Epoch 2/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why is the model pretty much perfect?\n",
        "The transformer changes the input data (e.g. they're no longer soundwaves by the time that they reaach the dense layers), but they still have some kind of understanding. The dense layers at this point are learning the transformer and not necessarily the data so the output would be the same with or without the transformer in terms of the model overfitting.\n",
        "\n",
        "Moral of the story is that transformers are very new and people don't really understand how to use them. Miracle of the transformer - it was added in ChatGBT3. As you add thousands of transformers (# of params), the performance exponentially improves. The \"miracle\" is that they haven't hit the point of diminishing returns - there doesn't seem to be a ceiling.\n",
        "\n",
        "But, climate yo. Massive water and electricity use for powering and cooling data centers. Also, privacy."
      ],
      "metadata": {
        "id": "Lod4qHi5lMBX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_AmHWtGTi-w7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}